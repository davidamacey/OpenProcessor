# =============================================================================
# triton-api Configuration Template
# =============================================================================
# Copy this file to .env: cp env.template .env
# Or run ./scripts/setup.sh to auto-generate based on your GPU
# =============================================================================

# =============================================================================
# GPU Configuration
# =============================================================================

# GPU Profile (auto-detected during setup)
# Options:
#   minimal  - 6-8GB VRAM (RTX 3060, RTX 4060) - core models only
#   standard - 12-24GB VRAM (RTX 3080, RTX 4090) - all models
#   full     - 48GB+ VRAM (A6000, A100) - maximum parallelism
GPU_PROFILE=standard

# Which GPU to use (0-indexed)
# Use nvidia-smi to see available GPUs
GPU_ID=0

# =============================================================================
# Performance Settings (auto-configured based on GPU_PROFILE)
# =============================================================================

# Number of uvicorn workers for the FastAPI service
# Adjust based on CPU cores and memory
# Recommended: minimal=8, standard=16, full=32
TRITON_WORKERS=16

# Maximum batch size for inference
# Lower values use less VRAM but reduce throughput
# Recommended: minimal=16, standard=32, full=64
MAX_BATCH_SIZE=32

# Shared memory size for Docker containers
# Used for inter-process communication
# Recommended: minimal=4g, standard=6g, full=8g
SHM_SIZE=6g

# =============================================================================
# OpenSearch Vector Database
# =============================================================================

# JVM heap size for OpenSearch
# Affects search performance and memory usage
# Recommended: minimal=1g, standard=2g, full=4g
OPENSEARCH_HEAP=2g

# =============================================================================
# Port Configuration
# =============================================================================
# Change these if ports conflict with other services on your system

# Main API port (FastAPI service - all endpoints)
API_PORT=4603

# Triton Inference Server ports
TRITON_HTTP=4600
TRITON_GRPC=4601
TRITON_METRICS=4602

# Monitoring stack
PROMETHEUS_PORT=4604
GRAFANA_PORT=4605
LOKI_PORT=4606

# Vector database
OPENSEARCH_PORT=4607
OPENSEARCH_DASHBOARDS_PORT=4608

# =============================================================================
# Development Settings
# =============================================================================

# Enable debug mode (verbose logging)
# DEBUG=1

# Log level for uvicorn
# Options: critical, error, warning, info, debug, trace
# LOG_LEVEL=info
